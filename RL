import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import requests
import matplotlib.pyplot as plt

#QLearning Algorithm for Optimzal Policy
class QLearningPolicy:
    
    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.9):
        self.q_table = np.zeros((state_size, action_size))
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor

        
    def update_q_value(self, state, action, reward, next_state):
        max_future_q = np.max(self.q_table[next_state])
        current_q = self.q_table[state, action]
        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount_factor * max_future_q)
        self.q_table[state, action] = new_q

        
    def choose_action(self, state, epsilon=0.1):
        if np.random.rand() < epsilon:  
            return np.random.randint(self.q_table.shape[1])
        else:  
            return np.argmax(self.q_table[state])

# SGD for Optimal Policy
# This function is not complete
class PolicyGradient(nn.Module):
    
    def __init__(self, state_size, action_size, learning_rate=0.01):
        super(PolicyGradient, self).__init__()
        self.fc = nn.Linear(state_size, action_size)
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)

class ReinforcementLearning:
    
    def __init__(self, T, networks_1, networks_2, actions, lambda_param=0.5, state_size=None, action_size=3, policy_type='q_learning'):
        self.T = T  # Time horizon
        self.networks_1 = networks_1  # List of snapshots for Network 1
        self.networks_2 = networks_2  # List of snapshots for Network 2
        self.actions = actions  # List of possible actions
        
        # Initialize alignment tracking
        self.alignments = [None] * T  # Alignment for each time step, initially set to None
        
        # Other parameters (e.g., learning rate, discount factor)
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        self.lambda_param = lambda_param
        self.policy_type = policy_type
        
        # Initialize the appropriate policy
        if policy_type == 'q_learning':
            self.policy = QLearningPolicy(state_size=state_size, action_size=action_size)
        elif policy_type == 'policy_gradient':
            self.policy = PolicyGradient(state_size=state_size, action_size=action_size)
        else:
            raise ValueError(f"Unknown policy type: {policy_type}")
    
    
    def edge_conservation_score(self, t, alignment):
        
        W1_t = self.networks_1[t]  # Weight matrix for Network 1 at time t
        W2_t = self.networks_2[t]  # Weight matrix for Network 2 at time t
        
        score = 0.0
        num_edges = 0
        for u1, v1 in alignment.items():
            for u2, v2 in alignment.items():
                if (u1, u2) in W1_t and (v1, v2) in W2_t:
                    score += W1_t[u1, u2] * W2_t[v1, v2]
                    num_edges += 1
        
        if num_edges > 0:
            score /= num_edges
        return score
    
    
    def temporal_consistency_score(self, t, alignment, previous_alignment):
        
        if t == 0 or previous_alignment is None:
            return 0
    
        consistent_pairs = 0
        total_pairs = len(alignment)
    
        for node, aligned_node in alignment.items():
            if node in previous_alignment and previous_alignment[node] == aligned_node:
                consistent_pairs += 1
                
        if total_pairs > 0:
            score = consistent_pairs / total_pairs
        else:
            score = 0.0
        return score
    
    
    
    def similarity_score(self, t, alignment, previous_alignment):

        edge_score = self.edge_conservation_score(t, alignment)
        temporal_score = self.temporal_consistency_score(t, alignment, previous_alignment)
    
        score = self.lambda_param * edge_score + (1 - self.lambda_param) * temporal_score
    
        return score
    
    
    
    def reward(self, t, alignment, previous_alignment):
        
        if t == 0:  # No previous alignment to compare with
            return 0.0

        current_score = self.similarity_score(t, alignment, previous_alignment)
        previous_score = self.similarity_score(t-1, previous_alignment, None)  # No previous alignment for t-1

        reward = current_score - previous_score
        return reward
    
    
    def cumulative_reward(self, alignments):
        cumulative_reward = 0.0
        for t in range(1, self.T):
            cumulative_reward += self.reward(t, alignments[t], alignments[t-1])

        return cumulative_reward
    
    def choose_action(self, state, epsilon=0.1):
        if self.policy_type == 'q_learning':
            return self.policy.choose_action(state, epsilon)
        elif self.policy_type == 'policy_gradient':
            state_tensor = torch.tensor(state, dtype=torch.float32)
            action_probs = self.policy(state_tensor)
            action = torch.multinomial(action_probs, 1).item()
            return action
    
    
    def update_policy(self, state, action, reward, next_state=None, log_probs=None, rewards=None):
        if self.policy_type == 'q_learning':
            self.policy.update_q_value(state, action, reward, next_state)
        elif self.policy_type == 'policy_gradient':
            self.policy.update_policy(log_probs, rewards)
            
    def update_alignment(self, alignment, action):

        new_alignment = alignment.copy()
        if action == 0:  # 'add-align'
            unaligned_nodes = [i for i in range(len(self.networks_1[0])) if i not in alignment]
            if unaligned_nodes:
                new_alignment[unaligned_nodes[0]] = unaligned_nodes[0]
        elif action == 1:  # 're-align'
            aligned_nodes = list(alignment.keys())
            if aligned_nodes:
                node_to_realign = aligned_nodes[0]
                new_alignment[node_to_realign] = (new_alignment[node_to_realign] + 1) % len(self.networks_2[0])
        # 'do-nothing' keeps the alignment unchanged
        
        return new_alignment
    
    def run_simulation(self):
        # Initial alignment
        alignment_t0 = {i: i for i in range(min(len(self.networks_1[0]), 5))}
        alignments = [alignment_t0]

        similarity_scores = []
        rewards = []
        cumulative_reward = 0.0
        cumulative_rewards = []

        #main loop
        for t in range(1, self.T):
            state = np.random.randint(len(self.networks_1[0]))  # Simplified state for demonstration
            action = self.choose_action(state)

            # Update alignment
            new_alignment = self.update_alignment(alignments[-1], action)
            alignments.append(new_alignment)

            # Calculate similarity score
            similarity_score = self.similarity_score(t, new_alignment, alignments[t-1])
            similarity_scores.append(similarity_score)

            # Calculate reward
            reward = self.reward(t, new_alignment, alignments[t-1])
            rewards.append(reward)

            # Update cumulative reward
            cumulative_reward += reward
            cumulative_rewards.append(cumulative_reward)

            # Update policy
            self.update_policy(state, action, reward)

        return alignments, similarity_scores, rewards, cumulative_rewards


